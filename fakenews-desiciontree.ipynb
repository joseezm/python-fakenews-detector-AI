{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = pd.read_excel('test_clean.xlsx')\n",
    "train_clean = pd.read_excel('train_clean.xlsx')\n",
    "train_label = train_clean['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>palo alto calif  years scorning political proc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>videos nodapl native american leaders vow stay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dont succeed try different sport tim tebow hei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42 mins ago 1 views 0 comments 0 likes for tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>sunday nbcs meet press house minority leader r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0           0  palo alto calif  years scorning political proc...      0\n",
       "1           2  videos nodapl native american leaders vow stay...      0\n",
       "2           3  dont succeed try different sport tim tebow hei...      1\n",
       "3           4  42 mins ago 1 views 0 comments 0 likes for tim...      1\n",
       "4           6  sunday nbcs meet press house minority leader r...      1"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_label[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARGANDO MODELO ENTRENADO DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Doc2Vec.load(\"d2v-30-30-1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.029579   -5.0857463   0.57676584 ... -4.890017    2.5237598\n",
      "  -5.125035  ]\n",
      " [ 0.3348972  -2.0161352   3.5392702  ... -0.9070313   0.25327632\n",
      "  -5.9858203 ]\n",
      " [-0.47013155 -1.2324988   3.689975   ... -0.49188724  4.5368466\n",
      "  -5.399334  ]\n",
      " ...\n",
      " [ 2.9820414   1.0170153   4.808458   ... -8.428259   -2.179974\n",
      "   0.72158355]\n",
      " [-2.1016812  -0.63795453  2.537954   ... -2.5955384  -0.36465353\n",
      "  -1.7016034 ]\n",
      " [ 1.8858492   1.709495    4.4647775  ...  3.2179825  -0.36528015\n",
      "  -1.5831916 ]]\n"
     ]
    }
   ],
   "source": [
    "text_vectors = model.docvecs.vectors_docs\n",
    "print(text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURACIÓN Y ENTRENAMIENTO DE DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=200,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=99, splitter='best')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=200, random_state= 99)\n",
    "tree.fit(text_vectors,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARGA Y PROCESAMIENTO DE DATOS DE PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = pd.read_excel('test_clean.xlsx')\n",
    "\n",
    "test_data = []\n",
    "vector_test_data = []\n",
    "\n",
    "for i in test_clean['text']:\n",
    "    test_data.append(word_tokenize(i))\n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "    vector_test_data.append(model.infer_vector(test_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.1571739 ,  1.6302937 ,  3.0468783 ,  2.3134942 ,  1.2223284 ,\n",
       "         -0.57258147, -0.412264  , -1.3346804 , -0.84787667, -1.5171411 ,\n",
       "         -0.64043033,  0.13324675,  0.8086485 ,  1.2461214 , -1.619541  ,\n",
       "         -0.15584725,  0.19151178,  0.77609354, -0.90637606, -0.10151214,\n",
       "         -1.1385549 , -2.2717714 , -0.43718415,  1.3122315 ,  0.7387772 ,\n",
       "         -1.1350175 , -2.708119  , -1.6149026 , -0.18913816, -1.5770895 ]],\n",
       "       dtype=float32),\n",
       " array([[-0.12410868,  1.2437177 ,  2.2509964 ,  0.4117859 ,  0.10370959,\n",
       "          2.2171578 ,  0.19246188,  1.4827666 ,  1.4080725 ,  1.0275875 ,\n",
       "         -0.8353295 ,  0.52497154, -0.4933222 , -1.1690015 ,  2.2792559 ,\n",
       "         -1.7166204 ,  2.2819834 ,  1.3859468 ,  1.120024  ,  0.48577282,\n",
       "         -1.2304682 , -0.8748603 , -0.5620775 ,  1.4239814 , -0.13019246,\n",
       "         -0.11638816, -1.9972913 ,  0.3436176 , -1.7005843 , -1.1959227 ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(vector_test_data)):\n",
    "    vector_test_data[i] = np.array(vector_test_data[i]).reshape(1,30)\n",
    "    \n",
    "vector_test_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASIFICANDO DATOS DE PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "test_results = []\n",
    "for i in vector_test_data:\n",
    "#     print(classifier.predict(i))\n",
    "    test_results.append(tree.predict(i)[0])\n",
    "    \n",
    "# predict = classifier.predict(vector_test_data[1])\n",
    "# print(predict)\n",
    "print(test_results[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULANDO PRESICIÓN OBTENIDA DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7185299295774648\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_results, test_clean['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
